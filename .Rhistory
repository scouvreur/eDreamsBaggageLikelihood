COMPANY <- NULL
for (i in 1:nrow(test)) {
COMPANY <- c(COMPANY, extractWebsite(test[i,"WEBSITE"]))
}
test$COMPANY <- as.factor(COMPANY)
rm(COMPANY, i, extractWebsite)
# Create family size variable
FAMILY_SIZE = train$ADULTS + train$CHILDREN + train$INFANTS
train$FAMILY_SIZE <- FAMILY_SIZE
FAMILY_SIZE = test$ADULTS + test$CHILDREN + test$INFANTS
test$FAMILY_SIZE <- FAMILY_SIZE
rm(FAMILY_SIZE)
# Create a utility function to extract if adult is travelling alone
extractAlone <- function(familysize) {
if (familysize > 1) {
return(0)
} else {
return(1)
}
}
# Create is alone variable
IS_ALONE <- NULL
for (i in 1:nrow(train)) {
IS_ALONE <- c(IS_ALONE, extractAlone(train[i,"FAMILY_SIZE"]))
}
train$IS_ALONE <- as.factor(IS_ALONE)
IS_ALONE <- NULL
for (i in 1:nrow(train)) {
IS_ALONE <- c(IS_ALONE, extractAlone(test[i,"FAMILY_SIZE"]))
}
test$IS_ALONE <- as.factor(IS_ALONE)
rm(IS_ALONE, i, extractAlone)
# Create distance category
train$DISTANCE_CAT <- factor(cut(train$DISTANCE, c(-1,4000,100000), labels = FALSE))
test$DISTANCE_CAT <- factor(cut(test$DISTANCE, c(-1,4000,100000), labels = FALSE))
# Adding labels to dataset
train$EXTRA_BAGGAGE <- factor(train$EXTRA_BAGGAGE,
levels = c("False","True"),
labels = c("NO_EXTRA_BAGGAGE", "EXTRA_BAGGAGE"))
train$IS_ALONE <- factor(train$IS_ALONE,
levels = c(0,1),
labels = c("NOT_ALONE", "ALONE"))
test$IS_ALONE <- factor(test$IS_ALONE,
levels = c(0,1),
labels = c("NOT_ALONE", "ALONE"))
# Variables not of interest removed
# Assuming there is no local variability between countries (UK, Italy, Spain etc.)
# big assumption though...
train <- subset(train, select = -c(TIMESTAMP, DEPARTURE:ARRIVAL,
TRAIN, PRODUCT, GDS,
NO_GDS, WEBSITE, SMS))
test <- subset(test, select = -c(TIMESTAMP, DEPARTURE:ARRIVAL,
TRAIN, PRODUCT, GDS,
NO_GDS, WEBSITE, SMS))
# Export data for Python XGBoost Machine Learning model
write.csv(train, file="train_xgboost.csv", row.names = FALSE, quote = FALSE)
write.csv(test, file="test_xgboost.csv", row.names = FALSE, quote = FALSE)
library(caret)
install.packages("caret")
library("caret", lib.loc="/usr/local/lib/R/3.4/site-library")
# Downsampling to balance both classes
down_train <- downSample(x = train[, -ncol(train)],
y = train$EXTRA_BAGGAGE)
View(down_train)
table(down_train$Class)
rm(down_train$Class)
down_train <- subset(down_train, select = -c(Class))
# Export data for Python XGBoost Machine Learning model
write.csv(down_train, file="train_xgboost.csv", row.names = FALSE, quote = FALSE)
write.csv(test, file="test_xgboost.csv", row.names = FALSE, quote = FALSE)
model <- glm(EXTRA_BAGGAGE ~ factor(HAUL_TYPE) + factor(TRIP_TYPE) +
DISTANCE + factor(DEVICE) + factor(COMPANY) +
FAMILY_SIZE + factor(IS_ALONE) + TRIP_LEN_DAYS,
data = down_train,
family = binomial(link = "logit"))
summary(model)
# 80/20 train/validation set split
validation <- train[40001:50000,]
validation$PREDICTION <- predict(model, validation, type="response")
ggplot(validation, aes(x = PREDICTION, fill = EXTRA_BAGGAGE)) + geom_density(alpha = 0.5)
rocobj <- roc(factor(validation$EXTRA_BAGGAGE), prediction, ci=TRUE)
rocobj <- roc(factor(validation$EXTRA_BAGGAGE), validation$PREDICTION, ci=TRUE)
plot(roc(factor(validation$EXTRA_BAGGAGE),
validation$PREDICTION,
ci=TRUE, direction="<"),
col="black",
print.auc=TRUE,
xlab="False Positive Rate",
ylab="True Positive Rate",
main="ROC Curve")
ggplot(validation, aes(x = PREDICTION, fill = EXTRA_BAGGAGE)) + geom_density(alpha = 0.5)
table(down_train$EXTRA_BAGGAGE)
rm(model)
rm(rocobj)
save.image("~/Dropbox/Documents/Projects/DataScience/eDreamsBaggageLikelihood/data.RData")
View(train)
# "It's difficult to make predictions, especially about the future."
# Niels Bohr, 1920
# Clear workspace variables
rm(list = ls())
cat("\014")
# Set working directory
setwd("~/Dropbox/Documents/Projects/DataScience/eDreamsBaggageLikelihood")
# Load libraries
library(pROC)
library(ggplot2)
library(caret)
# Load in data
train <- read.csv("train.csv", header = TRUE, sep = ";",
na.strings=c(""," ","NA"), stringsAsFactors = TRUE)
test <- read.csv("test.csv", header = TRUE, sep = ";",
na.strings=c(""," ","NA"), stringsAsFactors = TRUE)
# Fill in any missing values
train$DEVICE[is.na(train$DEVICE)] <- "OTHER"
test$DEVICE[is.na(test$DEVICE)] <- "OTHER"
# Check for missing data
length(train[!complete.cases(train),])
length(test[!complete.cases(test),])
# Reformatting data structure types
str(train)
str(test)
train$DISTANCE <- as.numeric(train$DISTANCE)
test$DISTANCE <- as.numeric(test$DISTANCE)
train$ARRIVAL <- as.Date(train$ARRIVAL, "%d/%b")
train$DEPARTURE <- as.Date(train$DEPARTURE, "%d/%b")
train$TRIP_LEN_DAYS <- as.integer(abs(difftime(train$ARRIVAL,
train$DEPARTURE,
units = "days")))
test$ARRIVAL <- as.Date(test$ARRIVAL, "%d/%b")
test$DEPARTURE <- as.Date(test$DEPARTURE, "%d/%b")
test$TRIP_LEN_DAYS <- as.integer(abs(difftime(test$ARRIVAL,
test$DEPARTURE,
units = "days")))
# Create a utility function to help with website extraction using UNIX grep
extractWebsite <- function(name) {
name <- as.character(name)
if (length(grep("ED", name)) > 0) {
return("EDREAMS")
} else if (length(grep("OP", name)) > 0) {
return("OPODO")
} else if (length(grep("GO", name)) > 0) {
return("GO_VOYAGE")
} else {
return("OTHER")
}
}
# Run function and clear workspace
COMPANY <- NULL
for (i in 1:nrow(train)) {
COMPANY <- c(COMPANY, extractWebsite(train[i,"WEBSITE"]))
}
train$COMPANY <- as.factor(COMPANY)
COMPANY <- NULL
for (i in 1:nrow(test)) {
COMPANY <- c(COMPANY, extractWebsite(test[i,"WEBSITE"]))
}
test$COMPANY <- as.factor(COMPANY)
rm(COMPANY, i, extractWebsite)
# Create family size variable
FAMILY_SIZE = train$ADULTS + train$CHILDREN + train$INFANTS
train$FAMILY_SIZE <- FAMILY_SIZE
FAMILY_SIZE = test$ADULTS + test$CHILDREN + test$INFANTS
test$FAMILY_SIZE <- FAMILY_SIZE
rm(FAMILY_SIZE)
# Create a utility function to extract if adult is travelling alone
extractAlone <- function(familysize) {
if (familysize > 1) {
return(0)
} else {
return(1)
}
}
# Create is alone variable
IS_ALONE <- NULL
for (i in 1:nrow(train)) {
IS_ALONE <- c(IS_ALONE, extractAlone(train[i,"FAMILY_SIZE"]))
}
train$IS_ALONE <- as.factor(IS_ALONE)
IS_ALONE <- NULL
for (i in 1:nrow(train)) {
IS_ALONE <- c(IS_ALONE, extractAlone(test[i,"FAMILY_SIZE"]))
}
test$IS_ALONE <- as.factor(IS_ALONE)
rm(IS_ALONE, i, extractAlone)
# Create distance category
train$DISTANCE_CAT <- factor(cut(train$DISTANCE, c(-1,4000,100000), labels = FALSE))
test$DISTANCE_CAT <- factor(cut(test$DISTANCE, c(-1,4000,100000), labels = FALSE))
# Adding labels to dataset
train$EXTRA_BAGGAGE <- factor(train$EXTRA_BAGGAGE,
levels = c("False","True"),
labels = c("NO_EXTRA_BAGGAGE", "EXTRA_BAGGAGE"))
train$IS_ALONE <- factor(train$IS_ALONE,
levels = c(0,1),
labels = c("NOT_ALONE", "ALONE"))
test$IS_ALONE <- factor(test$IS_ALONE,
levels = c(0,1),
labels = c("NOT_ALONE", "ALONE"))
save.image("~/Dropbox/Documents/Projects/DataScience/eDreamsBaggageLikelihood/data.RData")
## R code for statistical analysis on TOFPR dataset ##
# Clear variables and history, load libraries, set working dir ----
rm(list=ls())
cat("\014")
library(ggplot2)
library(ape, stats, ggpubr, corrplot, PerformanceAnalytics, plsdepot)
setwd("~/Dropbox/Documents/PhDCardiovascularScience/Deliverables/Statistics")
# Read in input data sets and match observations ----
df <- read.csv("TOFPR-stats.csv", header = TRUE, row.names = 2)
df <- subset(df, select = c(sex:years_since_TOF_repair))
surfvol <- read.csv("file.csv", header = TRUE, row.names = 1)
cl <- read.csv("CL_length_data.csv", header = TRUE, row.names = 1)
df <- merge(df, surfvol, by="row.names", all.x = TRUE)
rm(surfvol)
rownames(df) <- df$Row.names
df <- within(df, rm("Row.names"))
df <- merge(df, cl, by="row.names", all.x = TRUE)
rm(cl)
rownames(df) <- df$Row.names
df <- df[order(as.numeric(df$Row.names)),]
df <- within(df, rm("Row.names"))
df$scan_age_years <- as.numeric(df$scan_age_years)
df$MPA_RF_percent <- as.numeric(df$MPA_RF_percent)
pca_col_names <- NULL
for (i in 1:10) {
pca_col_names[i] <- paste("mode",i,sep = "")
}
rm(i)
pca <- read.table("Shape_vector.txt", sep = " ", col.names = pca_col_names)
plist <- read.table("patient_list.txt", sep = "", col.names = c("patient_num"))
pca <- merge(pca, plist, by ="row.names")
rm(plist)
rm(pca_col_names)
pca <- pca[order(as.numeric(pca$patient_num)),]
pca <- within(pca, rm("Row.names"))
rownames(pca) <- pca$patient_num
pca <- within(pca, rm("patient_num"))
df <- merge(df, pca, by ="row.names", all.x = TRUE)
df <- df[order(as.numeric(df$Row.names)),]
rownames(df) <- df$Row.names
df <- within(df, rm("Row.names"))
# For Tuesday 4th of Dec ----
# If data is normally distributed, it converts mode weigths
# to zscores and adds it to an new data.frame called PCA_zscores
p_PCA_mode_1 <- shapiro.test(pca$PCA_mode_1)
if (p_PCA_mode_1$p.value > 0.05) {
PCA_mode_1_zscore <- scale(pca$PCA_mode_1)
}
pca_zscore = data.frame()
PCA_zscores <- NULL
PCA_zscores$Row.names <- rownames(pca)
PCA_zscores$PCA_mode_1_zscore <- scale(pca$PCA_mode_1)
PCA_zscores <- cbind(PCA_mode_1_zscore, PCA_mode_2_zscore)
# Examine the structure of the dataframe
str(df)
# H1 - No correlation between BSA and RV+PA volume, RV+PA surface area, or center ----
bsa_vs_vol <- ggscatter(df, x = "BSA_m2", y = "RVOT_volume_mm3",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
bsa_vs_surf <- ggscatter(df, x = "BSA_m2", y = "RVOT_surfacearea_mm2",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
bsa_vs_lpa <- ggscatter(df, x = "BSA_m2", y = "CL_len_RV_LPA_mm",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
bsa_vs_rpa <- ggscatter(df, x = "BSA_m2", y = "CL_len_RV_RPA_mm",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
ggarrange(bsa_vs_vol,bsa_vs_surf,bsa_vs_lpa,bsa_vs_rpa,
labels = c("A", "B", "C", "D"),
ncol = 2, nrow = 2)
ggscatter(df, x = "RVEDV_mL_m2", y = "PCA_mode_3",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
ggscatter(df, x = "MPA_RF_percent", y = "mode3",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
ggscatter(df, x = "RVSV_mL_beat", y = "mode3",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
ggscatter(df, x = "dias_BP_mmHg", y = "mode1",
add = "reg.line",               # Add regression line
conf.int = TRUE) +              # Add confidence interval
stat_cor(label.x = 1)           # Add correlation coefficient
# Test data for normality
sw_BSA <- shapiro.test(df$BSA_m2)
sw_RVPA_vol <- shapiro.test(df$RVOT_volume_mm3)
# Choose the type of test depending on if data is normally distributed
if ((sw_BSA$p.value > 0.05) && (sw_RVPA_vol$p.value > 0.05)) {
print("Normally distributed data")
cor.test(df$BSA_m2, df$RVOT_volume_mm3, method = "pearson", use = "complete.obs")
} else {
print("Non-normally distributed data")
cor.test(df$BSA_m2, df$RVOT_volume_mm3, method = "kendall", use = "complete.obs")
cor.test(df$BSA_m2, df$RVOT_volume_mm3, method = "spearman", use = "complete.obs")
}
# Perform hierarchical clustering ----
hc <- hclust(dist(as.matrix(pca)))
# Cut the dendrogram into 5 clusters
colors = c("red", "blue", "green", "black", "pink")
Group = cutree(hc, k = 5)
pdf('plot.pdf', 12, 8)
plot(as.phylo(hc), type = "fan", tip.color = colors[Group],
label.offset = 1, cex = 0.75)
dev.off()
pdf('plot0.pdf', 12, 8)
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = 1.0)
rect.hclust(hc, k = 5, border = "blue")
dev.off()
Group <- data.frame(Group)
df <- merge(df, Group, by="row.names", all.x = TRUE)
rownames(df) <- df$Row.names
df <- df[order(as.numeric(df$Row.names)),]
df <- within(df, rm("Row.names"))
df$patient_num <- rownames(df)
df$sex <- as.factor(df$sex)
df$Group <- as.factor(df$Group)
bp <- ggbarplot(data=subset(df, !is.na(Group)), x = "patient_num", y = "MPA_RF_percent",
fill = "Group",             # change fill color by cyl
color = "white",            # Set bar border colors to white
palette = "jco",            # jco journal color palett. see ?ggpar
sort.val = "asc",           # Sort the value in ascending order
sort.by.groups = TRUE,      # Sort inside each group
x.text.angle = 90           # Rotate vertically x axis texts
)
bp
setwd("~/Dropbox/Documents/PhDCardiovascularScience/Deliverables/Statistics")
library(ape, stats, ggpubr, corrplot, PerformanceAnalytics, plsdepot)
# Read in input data sets and match observations ----
df <- read.csv("TOFPR-stats.csv", header = TRUE, row.names = 2)
df <- subset(df, select = c(sex:years_since_TOF_repair))
surfvol <- read.csv("file.csv", header = TRUE, row.names = 1)
cl <- read.csv("CL_length_data.csv", header = TRUE, row.names = 1)
df <- merge(df, surfvol, by="row.names", all.x = TRUE)
rm(surfvol)
rownames(df) <- df$Row.names
df <- within(df, rm("Row.names"))
df <- merge(df, cl, by="row.names", all.x = TRUE)
rm(cl)
rownames(df) <- df$Row.names
df <- df[order(as.numeric(df$Row.names)),]
df <- within(df, rm("Row.names"))
df$scan_age_years <- as.numeric(df$scan_age_years)
df$MPA_RF_percent <- as.numeric(df$MPA_RF_percent)
pca_col_names <- NULL
for (i in 1:10) {
pca_col_names[i] <- paste("mode",i,sep = "")
}
rm(i)
pca <- read.table("Shape_vector.txt", sep = " ", col.names = pca_col_names)
plist <- read.table("patient_list.txt", sep = "", col.names = c("patient_num"))
pca <- merge(pca, plist, by ="row.names")
rm(plist)
rm(pca_col_names)
pca <- pca[order(as.numeric(pca$patient_num)),]
pca <- within(pca, rm("Row.names"))
rownames(pca) <- pca$patient_num
pca <- within(pca, rm("patient_num"))
View(df)
library("ape", lib.loc="/usr/local/lib/R/3.4/site-library")
library("plsdepot", lib.loc="/usr/local/lib/R/3.4/site-library")
library("pls", lib.loc="/usr/local/lib/R/3.4/site-library")
detach("package:pls", unload=TRUE)
library("PerformanceAnalytics", lib.loc="/usr/local/lib/R/3.4/site-library")
library("corrplot", lib.loc="/usr/local/lib/R/3.4/site-library")
library("ggpubr", lib.loc="/usr/local/lib/R/3.4/site-library")
## R code for statistical analysis on TOFPR dataset ##
# Clear variables and history, load libraries, set working dir ----
rm(list=ls())
cat("\014")
library(ggplot2)
library(ape, stats, ggpubr, corrplot, PerformanceAnalytics, plsdepot)
setwd("~/Dropbox/Documents/PhDCardiovascularScience/Deliverables/Statistics")
# Read in input data sets and match observations ----
df <- read.csv("TOFPR-stats.csv", header = TRUE, row.names = 2)
df <- subset(df, select = c(sex:years_since_TOF_repair))
surfvol <- read.csv("file.csv", header = TRUE, row.names = 1)
cl <- read.csv("CL_length_data.csv", header = TRUE, row.names = 1)
df <- merge(df, surfvol, by="row.names", all.x = TRUE)
rm(surfvol)
rownames(df) <- df$Row.names
df <- within(df, rm("Row.names"))
df <- merge(df, cl, by="row.names", all.x = TRUE)
rm(cl)
rownames(df) <- df$Row.names
df <- df[order(as.numeric(df$Row.names)),]
df <- within(df, rm("Row.names"))
df$scan_age_years <- as.numeric(df$scan_age_years)
df$MPA_RF_percent <- as.numeric(df$MPA_RF_percent)
pca_col_names <- NULL
for (i in 1:10) {
pca_col_names[i] <- paste("mode",i,sep = "")
}
rm(i)
pca <- read.table("Shape_vector.txt", sep = " ", col.names = pca_col_names)
pca <- subset(df, select = c(mode1:mode10))
# Clear workspace variables
rm(list = ls())
cat("\014")
# Set working directory
setwd("~/Dropbox/Documents/PhDCardiovascularScience/Deliverables/Statistics")
# Load libraries
library(ape, stats, ggpubr, ggplot2, corrplot, PerformanceAnalytics, plsdepot)
# Data processing/massaging
df <- read.csv("populationTOF.csv", header = TRUE,
sep = ";", na.strings=c(""," ","NA"))
rownames(df) <- df$numPatient
df$homograft <- as.factor(df$homograft)
df <- subset(df, select = -c(imAcquired:DOB, DOS, hospitalNum, dateECG,
repairComment:monthsToEvent, comment:repairYear))
View(df)
pca <- subset(df, select = c(mode1:mode10))
View(pca)
# Perform hierarchical clustering ----
hc <- hclust(dist(as.matrix(pca)))
pdf('plot0.pdf', 12, 8)
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = 1.0)
rect.hclust(hc, k = 3, border = "blue")
dev.off()
pdf('plot0.pdf', 10, 6)
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = 1.0)
rect.hclust(hc, k = 3, border = "blue")
dev.off()
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
rect.hclust(hc, k = 3, border = "blue")
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
rect.hclust(hc, k = 3, border = "blue")
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
pdf('plot0.pdf', 10, 6)
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
dev.off()
# Cut the dendrogram into clusters
colors = c("red", "blue", "green")
Group = cutree(hc, k = 3)
plot(as.phylo(hc), type = "fan", tip.color = colors[Group],
label.offset = 1, cex = 0.75)
a?s.phylo()
?as.phylo()
?plot()
?plo
?plot
library(dendextend)
install.packages("dendextend")
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
pdf('plot00.pdf', 10, 6)
plot(hc, main = "", sub = "", xlab = "",
ylab="Height", hang = -1, cex = .8)
rect.hclust(hc, k = 3, border = "blue")
dev.off()
# "It's difficult to make predictions, especially about the future."
# Niels Bohr, 1920
# Clear workspace variables
rm(list = ls())
cat("\014")
# Set working directory
setwd("~/Dropbox/Documents/Projects/DataScience/eDreamsBaggageLikelihood")
# Load libraries
library(pROC)
library(ggplot2)
library(caret)
# "It's difficult to make predictions, especially about the future."
# Niels Bohr, 1920
# Clear workspace variables
rm(list = ls())
cat("\014")
# Set working directory
setwd("~/Dropbox/Documents/Projects/DataScience/eDreamsBaggageLikelihood")
# Load libraries
library(pROC)
library(ggplot2)
library(caret)
# Load in data
train <- read.csv("train.csv", header = TRUE, sep = ";",
na.strings=c(""," ","NA"), stringsAsFactors = TRUE)
test <- read.csv("test.csv", header = TRUE, sep = ";",
na.strings=c(""," ","NA"), stringsAsFactors = TRUE)
# Fill in any missing values
train$DEVICE[is.na(train$DEVICE)] <- "OTHER"
test$DEVICE[is.na(test$DEVICE)] <- "OTHER"
# Check for missing data
length(train[!complete.cases(train),])
length(test[!complete.cases(test),])
# Reformatting data structure types
str(train)
str(test)
train$DISTANCE <- as.numeric(train$DISTANCE)
test$DISTANCE <- as.numeric(test$DISTANCE)
train$ARRIVAL <- as.Date(train$ARRIVAL, "%d/%b")
train$DEPARTURE <- as.Date(train$DEPARTURE, "%d/%b")
train$TRIP_LEN_DAYS <- as.integer(abs(difftime(train$ARRIVAL,
train$DEPARTURE,
units = "days")))
test$ARRIVAL <- as.Date(test$ARRIVAL, "%d/%b")
test$DEPARTURE <- as.Date(test$DEPARTURE, "%d/%b")
test$TRIP_LEN_DAYS <- as.integer(abs(difftime(test$ARRIVAL,
test$DEPARTURE,
units = "days")))
View(train)
View(train)
