---
title: "eDream Odigeo Baggage Likelihood Model"
author: "Stéphane Couvreur"
date: "16/9/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pROC)
# library(lmtest, pROC)
load("data.RData")
```

\begin{center}
``The simulacrum is never that which conceals the truth — it is the truth which conceals that there is none. The simulacrum is true.''\\
Jean Baudrillard, \textit{Simulacra and Simulation}, 1988
\end{center}

# Exploratory Data Analysis
## Plotting and visualising the distributions of different variables

Overall proportion of people having booked extra baggage:

```{r, echo=FALSE}
round(prop.table(table(train$EXTRA_BAGGAGE))*100, digits = 1)
```

How to make the model as parsimonious as possible ? Let's see which values do not have such relevance

I think the best would be to try a good old mulitple logisitic regression model with several dummy variables, but omit data which at first you find useless.

Data which seems irrelevant at first sight:

\begin{itemize}
\item TIMESTAMP
\item DEPARTURE
\item ARRIVAL
\end{itemize}

As there is very strong class imbalance within the TRAIN booking binary variable (99.5% in the training set did not book a train).

```{r, echo=FALSE}
round(prop.table(table(train$TRAIN))*100, digits = 1)
```

Similarly within the PRODUCT variable (98.1% booked a Trip compared to a Dynpack) - both these variables were not considered.

```{r, echo=FALSE}
round(prop.table(table(train$PRODUCT))*100, digits = 1)
```

### Investigating further, comparing baggage selection rates among different variables

## Feature engineering

### Mapping of the booking company and encoding

It would be interesting to see if there are significant variations in baggage booking between eDreams (ED), Opodo (OP) or Go Voyage (GO) - should use a string operation on this

To simplify I assume that there is no local variability between bookings in UK, Italy, Spain, France etc.. Also, extracting different countries would just lead to a categorical factor variable with potentially many levels - which is not so good for a machine learning algorithm.

It seems that however there is nothing interesting there - the proportions are virtually exactly the same everywhere

```{r, echo=FALSE}
round(prop.table(table(train$COMPANY, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

The website variable can therefore be ommitted

Don't know what to do with GDS variables, I remove them for now and come back later

### Synthetic variable family size

After creating a synthetic variable combining ADULTS + CHILDREN + INFANTS called FAMILY_SIZE

Maybe those who pick SMS as an extra are more likely to pick other extras ? To investigate

Not much going on there actually

```{r, echo=FALSE}
round(prop.table(table(train$SMS, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

Could it be that with certain devices more customers book devices ?

```{r, echo=FALSE}
round(prop.table(table(train$DEVICE, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

Not so much actually really, a bit of a face value judgement but let's omit the DEVICE variable for now and investigate later if we have time

Adults travelling alone I would assume would be less likely to book luggage, but with one or more children much more likely to get luggage, especially with infants

Indeed from a small table you can see that:

```{r, echo=FALSE}
counts <- table(train$EXTRA_BAGGAGE, train$ADULTS)
barplot(counts, main="Adult Booking Distribution",
		xlab="Number of Adults in Booking", col=c("lightblue", "mistyrose"),
 		legend = rownames(counts))

round(prop.table(table(train$ADULTS, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

It seems that the more adults are travelling, the more likely they are to book luggage

```{r, echo=FALSE}
counts <- table(train$EXTRA_BAGGAGE, train$CHILDREN)
barplot(counts, main="Children Booking Distribution",
		xlab="Number of Children in Booking", col=c("lightcyan", "lavender"),
 		legend = rownames(counts))

round(prop.table(table(train$CHILDREN, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

```{r, echo=FALSE}
counts <- table(train$EXTRA_BAGGAGE, train$INFANTS)
barplot(counts, main="Infants Booking Distribution",
	    	xlab="Number of Infants in Booking", col=c("cornsilk","lightblue"),
 	    	legend = rownames(counts))

round(prop.table(table(train$INFANTS, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

```{r, echo=FALSE}
boxplot(FAMILY_SIZE ~ EXTRA_BAGGAGE, data=train, main="",
  	xlab="Baggage", ylab="Family size")

round(prop.table(table(train$FAMILY_SIZE, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

Increased family size also seems to bring with it increased probability of extra baggage selection.

### Synthetic variable adult alone

It would be interesting to see if the adults travelling alone tend to not book luggage as would be my initial assumption - we could create a binary variable IS_ALONE. Indeed from extracting this information it seems that we can improve our model as traevellers not alone have much more probability of booking luggage.

```{r, echo=FALSE}
round(prop.table(table(train$IS_ALONE, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

I would imagine that flight distance would account for a lot of the probability of luggage selection (high R2), as people who travel further I would assume need to carry more than if they are doing a short weekend trip within Europe

```{r, echo=FALSE}
counts <- table(train$EXTRA_BAGGAGE, train$HAUL_TYPE)
barplot(counts, main="Haul Booking Distribution",
	    	xlab="Haul Type", col=c("cornsilk","lightblue"),
 	    	legend = rownames(counts))

round(prop.table(table(train$HAUL_TYPE, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

There are quite significan differences here between groups. One can imagine that in intercontinental flights, the luggage from more premium companies will be complimentary so no extra is needed. And for domestic flights it makes sense - travelling at home you might need less luggage.

```{r, echo=FALSE}
counts <- table(train$EXTRA_BAGGAGE, train$TRIP_TYPE)
barplot(counts, main="Trip Booking Distribution",
	    	xlab="Trip Type", col=c("cornsilk","lightblue"),
 	    	legend = rownames(counts))

round(prop.table(table(train$TRIP_TYPE, train$EXTRA_BAGGAGE), 1)*100, digits = 1)
```

Interestingly, in round trips customers select extra baggage the least - perhaps they travel lighter as they know their belongings are at home. However much more take luggage on one ways (moving, expatriation or immigration perhaps ?) and even more on multi-destination trips.

As one would imagine, flight DISTANCE seems to follow a skeweved normal distribution with alot of short flights between 0-3000km and then drastic reductions from then onwards.

```{r, echo=FALSE}
hist(train$DISTANCE,
     main = "Air Travel Distance Distribution",
     xlab = "Distance [km]",
     ylab = "No of Bookings",
     col = "lightcyan",
     xlim = c(0,12000))

boxplot(DISTANCE ~ EXTRA_BAGGAGE, data=train, main="Flight Distance Data",
  	xlab="Baggage", ylab="Distance [km]")

```

# Building the model
## Logisitic Regression

Although our first assumption that number of adults was a predictor of baggage selection - indeed fitting it to our general linear model it would seem so as it's hightly significant in terms of p-value:

```{r, echo=FALSE}
model1 <- glm(EXTRA_BAGGAGE ~ DISTANCE + factor(HAUL_TYPE) + factor(TRIP_TYPE) +
			  ADULTS + CHILDREN + INFANTS,
              data = train,
              family = binomial(link = "logit"))
summary(model1)
```

However, considering adults childrens and infants as levels, it seems that having two children or one infant highly increases the change of selecting luggage. It might me interesting for the the sake of parsimony to remove the adult category.

```{r, echo=FALSE}
model2 <- glm(EXTRA_BAGGAGE ~ DISTANCE + factor(HAUL_TYPE) + factor(TRIP_TYPE) +
			  factor(ADULTS) + factor(CHILDREN) + factor(INFANTS),
              data = train,
              family = binomial(link = "logit"))
summary(model2)

# lrtest(model1, model2)
```

From the likelihood ratio test, it seems that there is strong evidence that 

If we remove the categories using the family size feature, we get:

```{r, echo=FALSE}
# 80/20 train/validation set split
validation <- train[40001:50000,]
train <- train[0:40000,]

model3 <- glm(EXTRA_BAGGAGE ~ DISTANCE + factor(HAUL_TYPE) + factor(TRIP_TYPE) + factor(DEVICE) +
              factor(COMPANY) + FAMILY_SIZE,
              data = train,
              family = binomial(link = "logit"))
summary(model3)
exp(cbind(odds=coef(model3), confint(model3)))

prediction <- predict(model3, validation, type="response")

roc_obj <- roc(factor(validation$EXTRA_BAGGAGE), prediction)
auc(roc_obj)

plot(roc(validation$EXTRA_BAGGAGE, prediction, direction="<"),
     col="black",
     xlab="False Positive Rate",
     ylab="True Positive Rate",
     main="ROC Curve")
```

We notice that looking at the odds ratio table that a unit increase in family size brings a 9.2% [95% CI 5.4 - 13.1] increase in probability of booking luggage as adjusting for several parameters.
We can therefore say with 95% confidence that the true odds ratio of booking luggage after adjusting for flight distance, haul type, trip type, company and booking device in our population lies between the range [X - X] with mean 

First iteration of the logistic regression model gives an AUC of:
```{r, echo=FALSE}
auc(roc_obj)
```

Overall one of the challenges of building this model is that there is strong class imbalance - indeed it might be interesting to try an xgboost model with the data encoded as levels rather than as dummy variables

Preliminary 80/20 train/validation split to have internal validation mechanism

## Linear model error estimation

To make sure that the AUC we get on the validation set we will also get on the test set (which is hidden from us), we should make a 5-fold cross validation where we can get a confidence interval on the AUC estimation



## Graph Boosted Machine with XGBoost

Code done in Python here


